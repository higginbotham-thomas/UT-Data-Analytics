---
title: "A Short Introduction to the caret Package Tutorial"
output: html_notebook
---

This notebook comes from A Short Introduction to the caret Package
By Max Kuhn, August 6, 2015

install caret:

```{r}
install.packages("caret", dependencies = c("Depends", "Suggests"))
```
Load libraries:
```{r}
library(caret)
library(mlbench)
```
Load the Sonar data
```{r}
data(Sonar)
```

The goal is to predict the two classes (M for metal cylinder or R for rock)

First split the data into two groups: a training set and a test set using the createDataPartition function

```{r}
set.seed(107)
inTrain <- createDataPartition(y = Sonar$Class,
                                            ## the outcome data are needed
                                            p = .75,
                                            ## The percentage of data in the
                                            ## training set
                                            list = FALSE)
                                            ## the format of the results

## The output is a set of integers for the rows of Sonar
## that belong in the training set.
str(inTrain)
```

By default, createDataPartition does a stratified random split of the data. To partition the data:

```{r}
training <- Sonar[ inTrain,]
testing  <- Sonar[-inTrain,]
nrow(training)
```

```{r}
nrow(testing)
```

Tune a partial least squares discriminant analysis (PLSDA) model over the number of PLS components that should be retained:

Basic syntax:
```{r}
plsFit <- train(Class ~ .,
                data = training,
                method = "pls",
                ## Center and scale the predictors for the training
                ## set and all future samples.
                preProc = c("center", "scale"))
```

By default, the function will tune over three values of each tuning parameter.

The simple bootstrap is used by default. Next example uses three repeats of 10-fold cross validation

If unspecified, overall accuracy and the Kappa statistic are computed. For regression models, root mean squared error and R2 are computed. Next example will be altered to estimate the area under the ROC curve, the sensitivity and specificity.

Use tuneLength or tuneGrid arguments to change the candidate values of the tuning parameters:

```{r}
plsFit <- train(Class ~ .,
                data = training,
                method = "pls",
                tuneLength = 15,
                ##this one is new compared to the first example
                preProc = c("center", "scale"))
```

Modify the resampling method using trainControl function.

method option controls the type of resampling, default is "boot"

repeatedcv used to specify repeated K-fold cross-validation, "repeats" for the number of repititions.

"number" controls K and defaults to 10

```{r}
ctrl <- trainControl (method = "repeatedcv",
                      repeats = 3)
plsFit <- train(Class ~ .,
                data = training,
                method = "pls",
                tuneLength = 15,
                trControl = ctrl,
                preProc = c("center","scale"))
```

Choose different measures of performance using additional arguments to trainControl. summaryFunction is used to pass a function that takes observed and predicted values and estimates some measure of performance. defaultSummary or twoClassSummary. twoClassSummary computes measures specific to two-class problemsn, like the area under the ROC curve, sensitivity, and specificity. Another option is required because ROC curve is based on the predicted class probablilityes. classProbs = TRUE is used to include that.

Because custome performance measures are used, we have to specify what criteria will be optimized. In train, use metric = "ROC" for that.

```{r}
ctrl <- trainControl(method = "repeatedcv",
                     repeats = 3,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)
plsFit <- train(Class ~ .,
                data = training,
                method = "pls",
                tuneLength = 15,
                trControl = ctrl,
                metric = "ROC",
                preProc = c("center", "scale"))
```

Show plsFit

```{r}
plsFit
```
Visualize the results
```{r}
plsClasses <- predict(plsFit, newdata = testing)
str(plsClasses)
```
```{r}
plsProbs <- predict(plsFit, newdata = testing, type = "prob")
head(plsProbs)
```
```{r}
plot(plsFit)
```
plot(plsFit) shows the relationship between the number of PLS components and the resampled estimate of the area under the ROC curve.

```{r}
confusionMatrix(data = plsClasses, testing$Class)
```

Try training another model
In this case, a regularized discriminant model

```{r}
## To illustrate, a custom grid is used
rdaGrid = data.frame(gamma = (0:4)/4, lambda = 3/4)
set.seed(123)
rdaFit <- train(Class ~ .,
                data = training,
                method = "rda",
                tuneGrid = rdaGrid,
                trControl = ctrl,
                metric = "ROC")
```


```{r}
rdaFit
```

```{r}
rdaClasses <- predict(rdaFit, newdata = testing)
```

```{r}
confusionMatrix(rdaClasses, testing$Class)
```

The resamples function can be used to collect, summarize and contrast the resampling results.

```{r}
resamps <- resamples(list(pls = plsFit, rda = rdaFit))
```

```{r}
summary(resamps)
```

Bland-Altman type plot to visualize the results
```{r}
xyplot(resamps, what = "BlandAltman")
```
```{r}
diffs <- diff(resamps)
summary(diffs)
```

Based on this analysis, the difference between the models is -0.018 ROC units (the RDA model is -.095) and the two–sided p–value for this difference is 0.4128.
